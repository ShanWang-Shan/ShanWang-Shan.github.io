<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shan Wang</title>
  
  <meta name="author" content="Shan Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shan Wang</name>
              </p>
              <p>I am a Computer Vision PhD student at <a href="https://www.anu.edu.au">Australian National University (ANU)</a>, under the guidance of Dr. <a href="https://people.csiro.au/N/C/Chuong-Nguyen">Chuong Nguyen</a> and <a href="http://users.cecs.anu.edu.au/~hongdong/">Prof. Hongdong Li</a>. My research focuses on Autonomous Driving and 3D reconstruction. Prior to joining ANU,  I accumulated 16 years of experience as an embedded software engineer in the automobile industry.
              </p>
              <p style="text-align:center">
                <a href="mailto:shan.wang@anu.edu.au">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=3xT-7nsAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="data/shan_CV">CV</a> &nbsp/&nbsp
                <a href="https://github.com/ShanWang-Shan">Github</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2023: One paper accepted to ICCV 2023</li>

                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table> -->



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/PureACL" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="#">
                <papertitle>View Consistent Purification for Accurate Cross-View Localization</papertitle>
              </a>
              <br>
              <strong>Shan Wang</strong>, Yanhao Zhang, Akhil Perincherry, Ankit Vora, and Hongdong Li
              <br>
              ICCV, 2023
              <br>
              <a href="#"> paper </a>  /
              <a href="#"> project page </a>
              <p></p>
              <p> This paper proposes a fine-grained self-localization method for outdoor robotics that utilizes a flexible number of onboard cameras and readily accessible satellite images. 
The proposed method addresses limitations in existing cross-view localization methods that struggle to handle noise sources such as moving objects and seasonal variations, achieving significant performance improvement. 
              </p>
            </td>
        </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/HomoFusion" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="#">
                <papertitle>Homography Guided Temporal Fusion for Road Line and Marking Segmentation</papertitle>
              </a>
              <br>
              <strong>Shan Wang</strong>, Chuong Nguyen, Jiawei Liu, Kaihao Zhang, Wenhan Luo, Yanhao Zhang, Sundaram Muthu, Fahira Afzalmaken and Hongdong Li
              <br>
              ICCV, 2023
              <br>
              <a href="#"> paper </a>  /
              <a href="#"> code </a>
              <p></p>
	      <p> Reliable segmentation of road lines and markings is critical to autonomous driving. Our work is motivated by the observations that road lines and markings are (1) frequently occluded in the presence of moving vehicles, shadow, and glare and (2) highly structured with low intra-class shape variance and overall high appearance consistency. To solve these issues, we propose a Homography Guided Fusion (HomoFusion) module to exploit temporally-adjacent video frames for complementary cues facilitating the correct classification of the partially occluded road lines or markings.
              </p>
            </td>
        </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Calibration" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="#">
                <papertitle>Model Calibration in Dense Classification with Adaptive Label Perturbation</papertitle>
              </a>
              <br>
	      J Liu, C Ye, <strong>S Wang<strong>, R Cui, J Zhang, K Zhang, N Barnes
              <br>
              ICCV, 2023
              <br>
              <a href="https://arxiv.org/pdf/2307.13539.pdf"> paper </a>  /
              <a href="https://github.com/Carlisle-Liu/ASLP"> code </a>
              <p></p>
	      <p> .For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. 
              </p>
            </td>
        </tr>
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SIBCL" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161527">
                <papertitle>Satellite image based cross-view localization for autonomous vehicle</papertitle>
              </a>
              <br>
              <strong>Shan Wang</strong>, Yanhao Zhang, Akhil Perincherry, Ankit Vora, and Hongdong Li
              <br>
              ICCV, 2023
              <br>
              <a href="https://arxiv.org/abs/2207.13506"> paper </a>  /
              <a href="#"> code </a>
              <p></p>
	      <p> Existing spatial localization techniques for autonomous vehicles mostly use a pre-built 3D-HD map, often constructed using a survey-grade 3D mapping vehicle, which is not only expensive but also laborious. This paper shows that by using an off-the-shelf high-definition satellite image as a ready-to-use map, we are able to achieve cross-view vehicle localization up to a satisfactory accuracy, providing a cheaper and more practical way for localization. Our method is validated on KITTI and Ford Multi-AV Seasonal datasets as ground view and Google Maps as the satellite view. The results demonstrate the superiority of our method in cross-view localization with median spatial and angular errors within 1 meter and 1‚àò, respectively.
              </p>
            </td>
        </tr>
		
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVLNet" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Shi_CVLNet_Cross-View_Feature_Correspondence_Learning_for_Video-based_Camera_Localization_ACCV_2022_paper.pdf">
                <papertitle>CVLNet: Cross-View Semantic Correspondence Learning for Video-based Camera Localization </papertitle>
              </a>
              <br>
	      Yujiao Shi, Xin Yu, <strong>Shan Wang<strong>, and Hongdong Li
              <br>
              ACCV, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Shi_CVLNet_Cross-View_Feature_Correspondence_Learning_for_Video-based_Camera_Localization_ACCV_2022_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/CVLNet"> code </a>
              <p></p>
              <p> This work addresses city-scale satellite image-based camera localization by using a sequence of ground-view images.
              </p>
            </td>
        </tr>

        </tbody></table>
				
        </tbody></table>
        <p align="center">
          <font size="2">
            Template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
          </font>
        </p>
      </td>
    </tr>
  </table>
</body>

</html>
