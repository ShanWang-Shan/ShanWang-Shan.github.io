<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shan Wang</title>
  
  <meta name="author" content="Shan Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shan Wang</name>
              </p>
              <p>I am a Computer Vision PhD student at <a href="https://www.anu.edu.au">Australian National University (ANU)</a>, under the guidance of Dr. <a href="https://people.csiro.au/N/C/Chuong-Nguyen">Chuong Nguyen</a> and Prof. <a href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>. My research focuses on Generative AI, Multimodal Learning, 3D Understanding, and autonomous perception. I bring a unique hybrid background: prior to my PhD, I spent 16 years as an Embedded Software Engineer in the automotive industry, architecting real-time multimedia and navigation systems for global OEMs (Toyota/Honda). Most recently, I interned at Amazon, where I developed diffusion models for Joint Shadow Generation and Relighting, and at NVIDIA, where I researched Hallucination Mitigation for MLLMs.
              </p>
              <p style="text-align:center">
                <a href="mailto:shan.wang@anu.edu.au">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=3xT-7nsAAAAJ">Google Scholar</a> &nbsp/&nbsp
		<a href="https://www.linkedin.com/in/shan-wang-375b3397/">Linkedin</a> &nbsp/&nbsp
                <a href="https://github.com/ShanWang-Shan">Github</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2023: One paper accepted to ICCV 2023</li>

                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table> -->



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/LGI.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/">
                <papertitle>Shadow and Relighting</papertitle>
              </a>
              <br>
              <strong>Shan Wang</strong>, Peixia Li, Chenchen Xu, Ziang Cheng, Jiayu Yang, Hongdong Li and Pulak Purkait
              <br>
              Wait for review results
              <br>
              <a href="https://arxiv.org/pdf/"> paper </a> 
              <p></p>
              <p> Wait for review results </p>
            </td>
        </tr>
			
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/GACD.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2509.03113">
                <papertitle>Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection</papertitle>
              </a>
              <br>
              <strong>Shan Wang</strong>, Maying Shen, Nadine Chang, Chuong Nguyen, Hongdong Li and Jose M Alvarez
              <br>
              Arxiv
              <br>
              <a href="https://arxiv.org/pdf/2509.03113"> paper </a> 
              <p></p>
              <p> Multimodal large language models (MLLMs) achieve strong performance across diverse tasks but remain prone to hallucinations, where outputs are not grounded in visual inputs. This issue can be attributed to two main biases: text‚Äìvisual bias, the overreliance on prompts and prior outputs, and co-occurrence bias, spurious correlations between frequently paired objects. We propose Gradient-based Influence-Aware Constrained Decoding (GACD), an inference-based method, that addresses both biases without auxiliary models, and is readily applicable to existing models without finetuning. The core of our approach is bias estimation, which uses first-order Taylor gradients to understand the contribution of individual tokens‚Äîvisual features and text tokens‚Äîto the current output. Based on this analysis, GACD mitigates hallucinations through two components: (1) suppressing spurious visual features correlated with the output objects, and (2) rebalancing cross-modal contributions by strengthening visual features relative to text. Experiments across multiple benchmarks demonstrate that GACD effectively reduces hallucinations and improves the visual grounding of MLLMs outputs.
			  </p>
            </td>
        </tr>
			
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/VFA.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_View_From_Above_Orthogonal-View_aware_Cross-view_Localization_CVPR_2024_paper.pdf">
                <papertitle>View from above: Orthogonal-view aware cross-view localization</papertitle>
              </a>
              <br>
              <strong>Shan Wang</strong>, Chuong Nguyen, Jiawei Liu, Yanhao Zhang, Sundaram Muthu, Fahira Afzalmaken, Kaihao Zhang and Hongdong Li
              <br>
              CVPR, 2024
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_View_From_Above_Orthogonal-View_aware_Cross-view_Localization_CVPR_2024_paper.pdf"> paper </a>
              <p></p>
              <p> This paper presents a novel aerial-to-ground feature aggregation strategy, tailored for the task of cross-view image-based geo-localization. Conventional vision-based methods heavily rely on matching ground-view image features with a pre-recorded image database, often through establishing planar homography correspondences via a planar ground assumption. As such, they tend to ignore features that are off-ground and not suited for handling visual occlusions, leading to unreliable localization in challenging scenarios. WeproposeaTop-to-Ground Aggregation (T2GA)module that capitalizes aerial orthographic views to aggregate features down to the ground level, leveraging reliable off-ground information to improve feature alignment. Furthermore, we introduce a Cycle Domain Adaptation (CycDA) loss that ensures feature extraction robustness across domain changes. Additionally, an Equidistant Re-projection (ERP) loss is introduced to equalize the impact of all keypoints on orientation error, leading to a more extended distribution of keypoints, which benefits orientation estimation. On both KITTI and Ford Multi-AV datasets, our method consistently achieves the lowest mean longitudinal and lateral translations across different settings and obtains the smallest orientation error when the initial pose is less accurate, a more challenging setting. Further, it can complete an entire route through continual vehicle pose estimation with initial vehicle pose given only at the starting point.
			  </p>
            </td>
        </tr>
			
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/PureACL.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_View_Consistent_Purification_for_Accurate_Cross-View_Localization_ICCV_2023_paper.html">
                <papertitle>View Consistent Purification for Accurate Cross-View Localization</papertitle>
              </a>
              <br>
              <strong>Shan Wang</strong>, Yanhao Zhang, Akhil Perincherry, Ankit Vora, and Hongdong Li
              <br>
              ICCV, 2023
              <br>
              <a href="https://arxiv.org/abs/2308.08110"> paper </a>  /
              <a href="https://shanwang-shan.github.io/PureACL-website/"> project page </a>
              <p></p>
              <p> This paper proposes a fine-grained self-localization method for outdoor robotics that utilizes a flexible number of onboard cameras and readily accessible satellite images. The proposed method addresses limitations in existing cross-view localization methods that struggle to handle noise sources such as moving objects and seasonal variations, achieving significant performance improvement. 
              </p>
            </td>
        </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/HomoFusion.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Homography_Guided_Temporal_Fusion_for_Road_Line_and_Marking_Segmentation_ICCV_2023_paper.html">
                <papertitle>Homography Guided Temporal Fusion for Road Line and Marking Segmentation</papertitle>
              </a>
              <br>
              <strong>Shan Wang</strong>, Chuong Nguyen, Jiawei Liu, Kaihao Zhang, Wenhan Luo, Yanhao Zhang, Sundaram Muthu, Fahira Afzalmaken and Hongdong Li
              <br>
              ICCV, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Homography_Guided_Temporal_Fusion_for_Road_Line_and_Marking_Segmentation_ICCV_2023_paper.html"> paper </a>  /
              <a href="https://github.com/ShanWang-Shan/HomoFusion"> code </a>
              <p></p>
	      <p> Reliable segmentation of road lines and markings is critical to autonomous driving. Our work is motivated by the observations that road lines and markings are (1) frequently occluded in the presence of moving vehicles, shadow, and glare and (2) highly structured with low intra-class shape variance and overall high appearance consistency. To solve these issues, we propose a Homography Guided Fusion (HomoFusion) module to exploit temporally-adjacent video frames for complementary cues facilitating the correct classification of the partially occluded road lines or markings.
              </p>
            </td>
        </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Calibration.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2307.13539.pdf">
                <papertitle>Model Calibration in Dense Classification with Adaptive Label Perturbation</papertitle>
              </a>
              <br>
	      J Liu, C Ye, <strong>S Wang</strong>, R Cui, J Zhang, K Zhang, N Barnes
              <br>
              ICCV, 2023
              <br>
              <a href="https://arxiv.org/pdf/2307.13539.pdf"> paper </a>  /
              <a href="https://github.com/Carlisle-Liu/ASLP"> code </a>
              <p></p>
	      <p> For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. 
              </p>
            </td>
        </tr>
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SIGL.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161527">
                <papertitle>Satellite image based cross-view localization for autonomous vehicle</papertitle>
              </a>
              <br>
              <strong>Shan Wang</strong>, Yanhao Zhang, Ankit Vora, Akhil Perincherry, and Hongdong Li
              <br>
              ICRA, 2023
              <br>
              <a href="https://arxiv.org/abs/2207.13506"> paper </a>  /
              <a href="https://shanwang-shan.github.io/SIBCL-website/"> project page </a>
              <p></p>
	      <p> Existing spatial localization techniques for autonomous vehicles mostly use a pre-built 3D-HD map, often constructed using a survey-grade 3D mapping vehicle, which is not only expensive but also laborious. This paper shows that by using an off-the-shelf high-definition satellite image as a ready-to-use map, we are able to achieve cross-view vehicle localization up to a satisfactory accuracy, providing a cheaper and more practical way for localization. Our method is validated on KITTI and Ford Multi-AV Seasonal datasets as ground view and Google Maps as the satellite view. The results demonstrate the superiority of our method in cross-view localization with median spatial and angular errors within 1 meter and 1‚àò, respectively.
              </p>
            </td>
        </tr>
		
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVLNet" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Shi_CVLNet_Cross-View_Feature_Correspondence_Learning_for_Video-based_Camera_Localization_ACCV_2022_paper.pdf">
                <papertitle>CVLNet: Cross-View Semantic Correspondence Learning for Video-based Camera Localization </papertitle>
              </a>
              <br>
	      Yujiao Shi, Xin Yu, <strong>Shan Wang</strong>, and Hongdong Li
              <br>
              ACCV, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Shi_CVLNet_Cross-View_Feature_Correspondence_Learning_for_Video-based_Camera_Localization_ACCV_2022_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/CVLNet"> code </a>
              <p></p>
              <p> This work addresses city-scale satellite image-based camera localization by using a sequence of ground-view images.
              </p>
            </td>
        </tr>

        </tbody></table>
				
        </tbody></table>
        <p align="center">
          <font size="2">
            Template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
          </font>
        </p>
      </td>
    </tr>
  </table>
</body>

</html>
